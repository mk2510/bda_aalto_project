---
title: "BDA - Assignment 9"
author: "Anonymous"
header-includes:
   - \usepackage{listings}
output:
  pdf_document:
    toc: yes
    toc_depth: 1
urlcolor: blue
bibliography: reference.bib
---
```{r setup, include=FALSE}
# This chunk sets echo = TRUE as default, that is print all code.
# knitr::opts_chunk$set can be used to set other notebook generation options, too.
# include=FALSE inside curly brackets makes this block not be included in the pdf.
install.packages("remotes")
remotes::install_github("avehtari/BDA_course_Aalto",
        subdir = "rpackage", upgrade="never")
install.packages("aaltobda")
knitr::opts_chunk$set(echo = TRUE)
library(aaltobda)
library(rstan)
library(loo)
library(LaplacesDemon) # important for the density plots
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

# 1. Introduction

It's been almost 2 years since the covid-19 started in December 2019, and it's been 1 year since world vaccination has been started December 2020. Even though most of the countries rushing into vaccination and some countries are picking over 80% rate in vaccination, the pandemic seems unstoppable. One of the popular idea to stop the pandemic is that to achieve a 'herd-immunity threshold, which occurs when a large portion of a community (the herd) becomes immune to a disease, making the spread of disease from person to person unlikely. Immune individuals are unlikely to contribute to disease transmission, disrupting chains of infection, which stops or slows the spread of disease @wiki.
It is known that the herd-immunity threshold is achievable only with high vaccination rates, and many scientists had thought that once people started being immunized en masse, herd immunity would permit society to return to normal. Most estimates had placed the threshold at 60â€“70% of the population gaining immunity, either through vaccinations or past exposure to the virus @nature.
Down below shows the graph of population fully vaccinated by countries.
```{r}
knitr::include_graphics("./image/share-people-fully-vaccinated-covid_world.png")
```

As the high vaccination rate is a big part of ending pandemic by achieving herd-immunity, our group was interested in predicting how much high vaccination rate that countries will achieve in future. As we expect the cumulative vaccination rate graph will follow logit function, we set our model as a logit function. We picked 5 countries,which are Finland, Portugal, Japan, Germany,and Hongkong for our data. The cumulative vaccination graphs in these countries roughly follow the logit function and our main modeling idea is to see find which model the data fits well (seperate model or hierarchical model) and to predict the future vaccination rate.

Down below shows the barplot of population rate who received at least one dose of covid19 vaccine by countries in our interest.
```{r}
knitr::include_graphics("./image/share-people-vaccinated-covid.png")
```

Down below shows the barplot of population rate of fully vaccinated against covid19 by countries in our interest.
```{r}
knitr::include_graphics("./image/share-people-fully-vaccinated-covid_bar.png")
```

Down below shows the barplot of fully vaccinated population rate by countries in our interest.
```{r}
knitr::include_graphics("./image/share-people-fully-vaccinated-covid_plot.png")
```

### Relative work

# 2. Dataset

Datasets down below show cumulative covid vaccinations rate by countries( collected from [linked phrase](https://ourworldindata.org/covid-vaccinations)).Country  column shows the country of this data. X column is originally from the date when the country started vaccination to the recent date of vaccination. We normalized the date column by giving index 0 to number of date and deviding it with the number of date. As the time length of the vaccination differs by country, we uniformly picked the 222 datapoints from the datasets before normalize the X column. Y column shows the covid vaccination rate in the country. With the cumulative covid vaccination number, we devide it by 2 times population because most of the vaccines require 2 doses to be fully vaccinated. Therefore, dimension of all datasets are (222,3).

```{r}
#setwd('/Users/chuhyeongyeong/2021_period1/bayesian_data_analysis/bda_aalto_project')
setwd("/home/chooh1/notebooks/BDA/project")
finland <- read.csv("data/Finland_output.csv")
germany <- read.csv("data/Germany_output.csv")
hongkong <- read.csv('data/Hong Kong_output.csv')
portugal <- read.csv('data/Portugal_output.csv')
japan <- read.csv('data/Japan_output.csv')
```

```{r}
cat('dimenstion of Finland dataset: ',dim(finland),'\n')
cat('dimenstion of Germany dataset: ',dim(germany),'\n')
cat('dimenstion of HongKong dataset: ',dim(hongkong),'\n')
cat('dimenstion of Portugal dataset: ',dim(portugal),'\n')
cat('dimenstion of Japan dataset: ',dim(japan),'\n')
```


```{r}
total= rbind(finland,germany,hongkong,portugal,japan)
ggplot(data = total, aes(x = X, y = Y, color = Country)) +
    geom_line() +
    ggtitle("Cumulative covid19 vaccination rate") +
    xlab("time") + ylab("vaccination rate")
```

# 3. Mathematical Models and Stan code

## Seperated Model

$$
\begin{aligned}
y_{i j} \mid \mu_i, \sigma &\sim \operatorname{Normal}\left(\mu_i, \sigma\right) \\
\mu_i &\sim \operatorname{logit}(\alpha_i, \beta_i)\\
\sigma &\sim \text{inv}\chi^2(1)\\
\\
\alpha_i &\sim \text{inv}\chi^2(1) \\
\beta_i &\sim N(0.5,1) \\
\end{aligned}
$$

We will first try to build the seperated model in stan

```{r}
seperated_model <- "
functions {
  real[] logit_transform(real[] x, real k, real x0) {
    int N = size(x);
    real xtemp[N];
    for (i in 1:N){
      xtemp[i] = 1 / (1 + exp(-k * (x[i] - x0)));
    }
     return xtemp;
  }
}

data {
    int<lower=0> J;
    int<lower=1> M;
    int<lower=0> N; // number of data points
    real x[M,N]; // observation year
    real y[M,N]; // observation number of drowned
    real xpred;  // prediction year
}

parameters {
  real alpha[M];
  real beta[M];
  real<lower = 0> sigma;
}

transformed parameters {
  real mu[M,N];
  for (i in 1:M){
    mu[i,] = logit_transform(x[i,],alpha[i], beta[i]);
  }
}

model {
  for (i in 1:M){
    // as prior we will change it to the same values
    alpha[i] ~ inv_chi_square(1);
    beta[i] ~ normal(0.5,1);
  }

  sigma ~ inv_chi_square(1);

  //likelihood
  for (i in 1:M){
    y[i,] ~ normal(mu[i,], sigma);
  }
}

generated quantities {
  vector[N] log_lik[M];
  real ypred[M];
  real t[1];
  t[1] = xpred;
  for (i in 1:M){
    ypred[i] = normal_rng(logit_transform(t,alpha[i], beta[i])[1], sigma);
  }

  //log-likelihood
  for (i in 1:M) {
    for (j in 1:N) {
    log_lik[i,j] = normal_lpdf(y[i,j] | mu[i,j], sigma);
    }
  }

}

"
```


## Hirachical Model

$$
\begin{aligned}
y_{i j} \mid \mu_i, \sigma &\sim \operatorname{Normal}\left(\mu_i, \sigma\right) \\
\mu_i &\sim \operatorname{logit}(\alpha_i, \beta_i)\\
\sigma & \sim \operatorname{Inv}-\chi^{2}(1) \\
\\
\beta_{j}\mid \mu_{\beta}, \sigma_{\beta} &\sim \operatorname{Normal}(\mu_{\beta}, \sigma_{\beta})\\
\alpha_{j} \mid \sigma_{\alpha} & \sim \operatorname{Inv}-\chi^{2}\left(\sigma_{\alpha}\right) \\
\\
\mu_{\beta} & \sim \operatorname{Normal}(0,1)\\
\sigma_{\beta} & \sim \operatorname{Inv}-\chi^{2}(1) \\
\sigma_{\alpha} & \sim \operatorname{Inv}-\chi^{2}(1) \\
\end{aligned}
$$



Now we will build the hierarchical model in stan:
```{r}
hierarchical_model <- "
functions {
  real[] logit_transform(real[] x, real k, real x0) {
    int N = size(x);
    real xtemp[N];
    for (i in 1:N){
      xtemp[i] = 1 / (1 + exp(-k * (x[i] - x0)));
    }
     return xtemp;
  }
}

data {
    int<lower=1> M; //number of country
    int<lower=0> N; // number of data points
    real x[M,N]; //
    real y[M,N]; //
    real xpred;  // prediction year
}

parameters {
  real alpha[M];
  real beta[M];
  real<lower = 0> sigma;
  real<lower = 0> hyper_sigma;
  real hyper_mu;
  real<lower = 0> hyper_alpha;
}

transformed parameters {
  real mu[M,N];
  for (i in 1:M){
    mu[i,] = logit_transform(x[i,],alpha[i], beta[i]);
  }
}

model {
    hyper_mu ~ normal(0, 1);
    hyper_sigma ~ inv_chi_square(10);
    hyper_alpha ~ inv_chi_square(10);

  for (i in 1:M){
    // as prior we will change it to the same values
    alpha[i] ~ inv_chi_square(hyper_alpha);
    beta[i] ~ normal(hyper_mu,hyper_sigma);
  }

  sigma ~ inv_chi_square(1);

  //likelihood
  for (i in 1:M){
    y[i,] ~ normal(mu[i,], sigma);
  }
}

generated quantities {
  vector[N] log_lik[M];
  real ypred[M];
  real t[1];
  t[1] = xpred;
  for (i in 1:M){
    ypred[i] = normal_rng(logit_transform(t,alpha[i], beta[i])[1], sigma);
  }

  //log-likelihood
  for (i in 1:M) {
    for (j in 1:N) {
    log_lik[i,j] = normal_lpdf(y[i,j] | mu[i,j], sigma);
    }
  }

}

"
```

# 4. Prior selection

For both models we choose to use weakly informative priors.
We will first present all prior distribuions in plots, to visualize better their properties.

```{r}
x <- seq(from=0.1, to=5, by=0.01)
plot(x, dinvchisq(x,1,1), ylim=c(0,1), type="l", main="inv-chi-square(1) PDF",
     ylab="density", col="red")
```
```{r}
x <- seq(from=-5, to=5, by=0.01)
plot(x, dnorm(x, 0.5, 1), ylim=c(0,1), type="l", main="Normal(0.5,1) PDF",
     ylab="density", col="red")
```
```{r}
x <- seq(from=-5, to=5, by=0.01)
plot(x, dnorm(x, 0, 1), ylim=c(0,1), type="l", main="Normal(0,1) PDF",
     ylab="density", col="red")
```

## Prior choice of the seperated model

For the seperated model, the $\alpha$ prior needed to fullfill two criteria. First it needed to be a positive number, as we can expect from the vaccination, that the rate is rising and not dropping. Also, as the vaccination should follow roughly a logit function, as it can be observed from the data, we can expect to have a gradient, which is more likely around $1$ than bigger than $50$. Hence the prior of $\operatorname{Inv}-\chi^{2}(1)$ was choosen.

For our $\beta$ prior estimation we know, that it should be in the range of $[0,1]$, as this is the range of our x-Data. Therefore we choose $N(0.5,1)$.

The variance of our y-sampling should be a positive number. As we can also expect it to be quite small, as we want our model following the line quite tightly, we choose here as well $\operatorname{Inv}-\chi^{2}(1)$.

## Prior choices of the hirachical model

The hirachical model has the following priors. The $\alpha$- prior still has the same distribution function: $\operatorname{Inv}-\chi^{2}(\cdot)$, with the same reasoning as for the seperated model. Here however, the parameter for the function gets sampled as well. With the same reasoning about the order of magnitude of our parameter we choose $\operatorname{Inv}-\chi^{2}(1)$ as a suitable weakly informative prior.
As we can see in the plot above, the PDF of the probability distribution has nearly all amount of its mass in the interval of $[0,10]$

The $\beta$ prior is again, like in the seperated model a normal distribution $N(\cdot,\cdot)$. For the first argument, we choose a normal distribution of $N(0,1)$, with the reasoning, that we expect it to be located somewhere in the interval of $[0,1]$, as this is the range of the data.
The prior of the variance is given by a $\operatorname{Inv}-\chi^{2}(1)$, as here as well we want to have a variance, which is not much larger, than our expected interval.


# 5. Stan code

The stan code was provided above with the describtion of the model

# 6. How to the Stan model was run, that is, what options were used. This is also more clear as combination of textual explanation and the actual code line


```{r}
# setwd("/Users/max/Documents/UniMac/Aalto/BDA/bda_aalto_project/data")
setwd("/home/chooh1/notebooks/BDA/project")
finland <- read.csv("data/Finland_output.csv")
germany <- read.csv("data/Germany_output.csv")
portugal <- read.csv("data/Portugal_output.csv")
hongkong <- read.csv("data/Hong Kong_output.csv")
japan <- read.csv("data/Japan_output.csv")
```


```{r, results='hide', error=FALSE}
#seperate model run
xData <- rbind(finland$X,germany$X,portugal$X,hongkong$X,japan$X) #(5,222)
yData <-rbind(finland$Y,germany$Y,portugal$Y,hongkong$Y,japan$Y) #(5,222)

sm <- rstan::stan_model(model_code = seperated_model)
stan_data <- list(
    J = 1,
    N = dim(xData)[2],
    M = dim(xData)[1],
    y = yData,
    x = xData,
    xpred = 1.1
)
model_separated <- rstan::sampling(sm, data = stan_data, warmup=3000, iter=4000)
fit_sm <- extract(model_separated, permuted = TRUE, inc_warmup = FALSE)
```

Below graph shows the posterior vaccination rate by countries in seperate model.

```{r}
Y_s = c(fit_sm$mu[3000,1,],fit_sm$mu[3000,2,],fit_sm$mu[3000,3,],
      fit_sm$mu[3000,4,],fit_sm$mu[3000,5,])
result_s = data.frame(Country=total$Country,X=total$X,Y=Y_s)
ggplot(data = result_s, aes(x =X, y = Y, color = Country)) +
    geom_line() +
    ggtitle("Sampled covid19 vaccination rate from the seperated model") +
    xlab("time") + ylab("posterior vaccination rate")
```


```{r}
#hierarchical model run
hm <- rstan::stan_model(model_code = hierarchical_model)
stan_data <- list(
    N = dim(xData)[2],
    M = dim(xData)[1],
    y = yData,
    x = xData,
    xpred = 1.1
)
model_hierarchical <- rstan::sampling(hm, data = stan_data, warmup=3000, iter=4000)
fit_hm <- extract(model_hierarchical, permuted = TRUE, inc_warmup = FALSE)
```

Below graph shows the posterior vaccination rate by countries in hierarchical model.

```{r}
Y_h = c(fit_hm$mu[3000,1,],fit_hm$mu[3000,2,],fit_hm$mu[3000,3,],
      fit_hm$mu[3000,4,],fit_hm$mu[3000,5,])
result_h = data.frame(Country=total$Country,X=total$X,Y=Y_h)
ggplot(data = result_h, aes(x =X, y = Y, color = Country)) +
    geom_line() +
    ggtitle("Sampled covid19 vaccination rate from the hierarchical model") +
    xlab("time") + ylab("posterior vaccination rate")
```

With the command ```rstan::stan_model``` we compile our stan models from a string vector and in the next step with ```rstan::sampling```
we create samples from the models. The given data are the datasets from section 2. As a warmup phase we use the generous length of $3000$ and as a total chain length $4000$. As the warmup lenght is longer than half of the chain length we made sure, that the samples are within our posterior distribution.

# 7.Convergence diagnostics (Rhat, ESS, divergences) and what was done if the convergence was not good with the first try.

We will use the build in tool from the stan library to analyse the convergence of our chains.

```{r}
#seperate model
sum_seperate <- summary(model_separated)
cat("Rhat of seperate model: \n")
sum_seperate$summary[1:11,10]
```

```{r}
#hierarchical
sum_hierarchical <- summary(model_hierarchical)
cat("Rhat of hierarchical model: \n")
sum_hierarchical$summary[1:11,10]
```

$$
\begin{aligned}
\hat{R} = \sqrt\frac{\hat{var^*}}{W}
\end{aligned}
$$

R hat is potential scale reduction factor. This value estimates how much the scale could reduce if N goes infinite. When N approches infinity, R hat approches 1. If R hat is bigger than 1.01, we should keep sampling.

As we can see here, the $\hat{R}$ values are less than 1, which means, that the chains converges. We will now run the ESS analysis, to further verify, that our $\hat{R}$ values are trustworthy. As mentioned in Verteri et al. 2019, a ESS value above 400 will indicate that the $\hat{R}$ value is reliable. We therefore now use the ESS method from  Verteri et al. 2019:

```{r}
cat("ESS of seperated model: \n")
sum_seperate$summary[1:11,9]
```
```{r}
cat("ESS of hiracical model: \n")
sum_hirachical$summary[1:11,9]
```

Here we can nicely see, that all ESS values are above 400, which indicates, that our $\hat{R}$ values are realiable. We can therefore conclude, that our chains converged.

# 8. Posterior predictive checks and what was done to improve the model.

```{r}
#the posterior expectation for parameter with a 90% credible interval
mu_pred_interval<-function(data, prob){
degree = length(data)-1
mu = mean(data)
sig = sqrt(var(data)*(1+1/length(data)))
error = qt(prob+(1-prob)/2,degree)*sig
lower = mu -error
upper = mu +error
interval = c(lower,upper)
return(interval)
}
```

## Seperate model
### The posterior expectations for alpha with a 90% credible interval 
```{r}
a1 = mu_pred_interval(data = fit_sm$alpha[,1], prob = 0.90)
a2 = mu_pred_interval(data = fit_sm$alpha[,2], prob = 0.90)
a3 = mu_pred_interval(data = fit_sm$alpha[,3], prob = 0.90)
a4 = mu_pred_interval(data = fit_sm$alpha[,4], prob = 0.90)
a5 = mu_pred_interval(data = fit_sm$alpha[,5], prob = 0.90)
cat()
cat()
```


### the posterior expectation for betas with a 90% credible interval
```{r}
b1 = mu_pred_interval(data = fit_sm$beta[,1], prob = 0.90)
b2 = mu_pred_interval(data = fit_sm$beta[,2], prob = 0.90)
b3 = mu_pred_interval(data = fit_sm$beta[,3], prob = 0.90)
b4 = mu_pred_interval(data = fit_sm$beta[,4], prob = 0.90)
b5 = mu_pred_interval(data = fit_sm$beta[,5], prob = 0.90)
```



# 9.Model comparison (e.g. with LOO-CV)

```{r,results='hide',error=FALSE}
#seperate model extract log_lik
l_like_s <- extract_log_lik(model_seperated)
loo_s <- loo(l_like_s)
elpd_s <- loo_s$estimates[1,1]

#hierarchical model extract log_lik
l_like_h <- extract_log_lik(model_hierarchical)
loo_h <- loo(l_like_h)
elpd_h <- loo_h$estimates[1,1]
```


```{r}
k_s <- loo_s$diagnostics$pareto_k
plot_s <- ggplot(data = data.frame(x = seq(1,length(k_s)) ,y = k_s))
plot_s + geom_point(aes(x = x, y = y, color = "blue"), alpha = 0.8) +
    theme_light() +
    xlab("") +
    ylab("k-value") +
    theme(legend.position="none")+
    ggtitle("K-values in seperate model")
```

```{r}
k_h <- loo_h$diagnostics$pareto_k
plot_h <- ggplot(data = data.frame(x = seq(1,length(k_h)) ,y = k_h))
plot_h + geom_point(aes(x = x, y = y, color = "blue"), alpha = 0.8) +
    theme_light() +
    xlab("") +
    ylab("k-value") +
    theme(legend.position="none")+
    ggtitle("K-values in hierarchical model")
```
```{r}
cat("Maximum k-values in seperate model is ",max(k_s),'\n')
cat("Maximum k-values in hierarchical model is ",max(k_h),'\n')
```

If the k < 0.7 we can say that PSIS-LOO estimates are reliable. Based on this,for seperate model and hierarchical model both, the \hat{k}-values are all below 0.5 which are good. Therefore, both  seperate model and hierarchicalmodels are reliable. However, the maximum  \hat{k}-values in seperate model is 0.216 and the maximum \hat{k}-values in hierarchical model is 0.176, so we could say the hierarchical model is more reliable.

####comparing PSIS-LOO estimates
```{r}
cat("PSIS-LOO estimates in seperate model",elpd_s,'\n')
cat("PSIS-LOO estimates in hierarchical model",elpd_h,'\n')
```

According to the PSIS-LOO estimate, hierarchical model has slightly higher value for the PSIS-LOO estimate than seperate model,so we could choose hierarchical model but still seperate model also have nice score in PSIS-LOO estimate.


# 10. Predictive performance assessment if applicable (e.g. classification accuracy) and evaluation of practical usefulness of the accuracy

We can now evaluate the predictive performance of our model.

## Seperate model

For the seperated model we have the following numbers:

```{r}
qplot(fit_sm$ypred[,1], geom="histogram") +
    xlab("") +
    ylab("") +
    ggtitle("Prediction of vaccination rate in close future,Finland")
```

```{r}
qplot(fit_sm$ypred[,2], geom="histogram") +
    xlab("") +
    ylab("") +
    ggtitle("Prediction of vaccination rate in close future, Germany")
```

```{r}
qplot(fit_sm$ypred[,3], geom="histogram") +
    xlab("") +
    ylab("") +
    ggtitle("Prediction of vaccination rate in close future, Portugal")
```

```{r}
qplot(fit_sm$ypred[,4], geom="histogram") +
    xlab("") +
    ylab("") +
    ggtitle("Prediction of vaccination rate in close future, Hongkong")
```

```{r}
qplot(fit_sm$ypred[,5], geom="histogram") +
    xlab("") +
    ylab("") +
    ggtitle("Prediction of vaccination rate in close future, Japan")
```

```{r}
cat("Mean of 4000 predictions for Finland: ", mean(fit_sm$ypred[,1]),'\n')
cat("Mean of 4000 predictions for Germany: ", mean(fit_sm$ypred[,2]),'\n')
cat("Mean of 4000 predictions for Portugal: ", mean(fit_sm$ypred[,3]),'\n')
cat("Mean of 4000 predictions for Hong Kong: ", mean(fit_sm$ypred[,4]),'\n')
cat("Mean of 4000 predictions for Japan: ", mean(fit_sm$ypred[,5]),'\n')
```


## Hierarchical model

Now we can evaluate the numbers for the hirachical model:

```{r}
qplot(fit_hm$ypred[,1], geom="histogram") +
    xlab("") +
    ylab("") +
    ggtitle("Prediction of vaccination rate in close future,Finland")
```

```{r}
qplot(fit_hm$ypred[,2], geom="histogram") +
    xlab("") +
    ylab("") +
    ggtitle("Prediction of vaccination rate in close future, Germany")
```

```{r}
qplot(fit_hm$ypred[,3], geom="histogram") +
    xlab("") +
    ylab("") +
    ggtitle("Prediction of vaccination rate in close future, Portugal")
```

```{r}
qplot(fit_hm$ypred[,4], geom="histogram") +
    xlab("") +
    ylab("") +
    ggtitle("Prediction of vaccination rate in close future, Hongkong")
```

```{r}
qplot(fit_hm$ypred[,5], geom="histogram") +
    xlab("") +
    ylab("") +
    ggtitle("Prediction of vaccination rate in close future, Japan")
```

```{r}
cat("Mean of 4000 predictions for Finland: ", mean(fit_hm$ypred[,1]),'\n')
cat("Mean of 4000 predictions for Germany: ", mean(fit_hm$ypred[,2]),'\n')
cat("Mean of 4000 predictions for Portugal: ", mean(fit_hm$ypred[,3]),'\n')
cat("Mean of 4000 predictions for Hong Kong: ", mean(fit_hm$ypred[,4]),'\n')
cat("Mean of 4000 predictions for Japan: ", mean(fit_hm$ypred[,5]),'\n')
```


We need to be very carefull about the prediction of the vaccination numbers. While it is nice having those predicitions, they are just usefull.  


# 12. Discussion of issues and potential improvements.


# 13. Conclusion what was learned from the data analysis.

Our main points can be summerized like follows:

1. We can use baysian statistics, to estimate the functional parameters for the logistic function. This can be verified, as our distribution converges and the logistic function is a close fit to the data

2. With some restrictions we can make cautious predrictions about the vaccination rate in the future

3. The models works best, when a country vaccination follows the logistic curve

# 14. Self-reflection of what the group learned while making the project.

Start the project early in case the whole group gets a flue during the last days 


TODO tomorrow:

SHow that for Nepal it doesn't work that well

Plot data and posterior in one plot to see how closely the data and model match